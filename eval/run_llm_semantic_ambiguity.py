"""
Stufe 6-LLM: Architecture Robustness — Stochastic Narrative Does Not
Disrupt Deterministic Hint Pipeline

Extends Stufe 6 (run_semantic_ambiguity.py) by replacing deterministic
AgentDInterpreter with AgentDLLMInterpreter (LLM narrative + deterministic
HintEncoder). Tests each model across the same variant matrix as Stufe 6.

IMPORTANT FRAMING:
This is a ROBUSTNESS test, not a capability test. Hint interpretation
is always deterministic (HintEncoder.decode), regardless of whether
the narrative is generated by a deterministic template or an LLM.
The question is: does LLM variability in narrative generation disrupt
the deterministic hint-decoding pipeline?

Secondary question: does LLM narrative quality affect deconstruction
quality? If the LLM adds spatial descriptions that help (or hurt) C's
navigation, this would be measurable via SR comparison.

Variants:
  modular_nod           — A+B+C, no D (baseline)
  modular_ond_interp    — A+B+C+D-interpreter, router-gated
  modular_ond_tb_interp — A+B+C+D-interpreter + tie-break
  baseline_mono         — Monolithic policy (no agents)
  ab_only               — Random walk with forward model

D agent per model:
  "deterministic"       — AgentDInterpreter (same as Stufe 6)
  LLM model name        — AgentDLLMInterpreter (LLM narrative + deterministic HintEncoder)

DEF Predictions (honestly framed):
  1. Pipeline robustness: D-Interpreter SR identical for LLM and
     deterministic (hint decoding is isolated from narrative).
  2. Interpretation isolation: hints_interpreted = 100% for all models
     (HintEncoder is deterministic, independent of LLM).
  3. Format-fallback isolation: even when LLM narrative fails format
     (llm_format_fallback), hint interpretation still works (separate
     pipelines).
  4. D > no-D holds model-independently: LLM-D-Interpreter > no-D
     on all configurations.
  5. Narrative quality as bonus signal: if LLM spatial descriptions
     affect C's navigation, SR may differ between LLM and deterministic.
     Both directions (better/worse) are informative.

Usage:
  python -m eval.run_llm_semantic_ambiguity                          # all models
  python -m eval.run_llm_semantic_ambiguity --model mistral:latest   # single model
  python -m eval.run_llm_semantic_ambiguity --n 10                   # quick test
  python -m eval.run_llm_semantic_ambiguity --no-deterministic       # skip control
"""

import argparse
import csv
import random
import sys
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional, Dict

from env.gridworld import GridWorld, GoalDef, HintCellDef
from env.coded_hints import CodedGridWorld, HintEncoder
from agents.agent_a import AgentA
from agents.agent_b import AgentB
from agents.agent_c import AgentC, GoalSpec
from agents.agent_d import AgentD
from agents.agent_d_interpreter import AgentDInterpreter
from agents.agent_d_llm_interpreter import AgentDLLMInterpreter
from llm.provider import OllamaProvider
from router.deconstruct import deconstruct_d_to_c
from router.router import Router, RouterConfig
from state.schema import ZC, ZA, ZD

from eval.baselines import baseline_monolithic_policy
from eval.stats import (
    confidence_interval_95,
    confidence_interval_proportion,
    compare_variants,
    format_comparison,
    mean,
)
from eval.llm_utils import (
    check_ollama_available,
    get_available_test_models,
    DEFAULT_MODELS,
)


ACTIONS = ("up", "down", "left", "right")


# ── Complexity Level Definitions ──────────────────────────────────────

@dataclass
class AmbiguityLevel:
    """Defines one test configuration for semantic ambiguity."""
    name: str
    difficulty: str
    width: int
    height: int
    goals: List[GoalDef]
    hint_cells: List[HintCellDef]
    n_random_obstacles: int
    max_steps: int


def _make_levels() -> List[AmbiguityLevel]:
    """Build the matrix of test levels (same as Stufe 6)."""
    levels = []

    for difficulty in ("easy", "medium", "hard"):
        # 5x5, 2 goals
        levels.append(AmbiguityLevel(
            name=f"5x5_2g_{difficulty}",
            difficulty=difficulty,
            width=5, height=5,
            goals=[GoalDef("A", (4, 4)), GoalDef("B", (4, 0))],
            hint_cells=[HintCellDef(pos=(0, 4), eliminates=[], hint_text="")],
            n_random_obstacles=0,
            max_steps=50,
        ))

        # 10x10, 2 goals
        levels.append(AmbiguityLevel(
            name=f"10x10_2g_{difficulty}",
            difficulty=difficulty,
            width=10, height=10,
            goals=[GoalDef("A", (9, 9)), GoalDef("B", (9, 0))],
            hint_cells=[HintCellDef(pos=(0, 9), eliminates=[], hint_text="")],
            n_random_obstacles=5,
            max_steps=100,
        ))

        # 10x10, 4 goals
        levels.append(AmbiguityLevel(
            name=f"10x10_4g_{difficulty}",
            difficulty=difficulty,
            width=10, height=10,
            goals=[
                GoalDef("A", (9, 9)), GoalDef("B", (9, 0)),
                GoalDef("C", (0, 9)), GoalDef("D", (5, 5)),
            ],
            hint_cells=[
                HintCellDef(pos=(0, 5), group_a=["A", "B"], group_b=["C", "D"]),
                HintCellDef(pos=(5, 0), group_a=["A", "C"], group_b=["B", "D"]),
            ],
            n_random_obstacles=5,
            max_steps=150,
        ))

    return levels


# ── Episode Result ───────────────────────────────────────────────────

@dataclass
class LLMAmbiguityResult:
    model: str              # "deterministic" or Ollama model name
    level: str
    difficulty: str
    variant: str
    goal_mode: str
    success: bool
    steps: int
    total_reward: float
    stay_rate: float
    d_triggers: int
    hints_collected: int
    hints_interpreted: int
    target_learned: bool
    mean_d_latency_ms: float
    format_fallback_rate: float
    narrative_mentions_hint: bool
    narrative_contradicts_hint: bool


# ── Episode Runner ───────────────────────────────────────────────────

VARIANTS_WITH_D = ("modular_ond_interp", "modular_ond_tb_interp")
VARIANTS_NO_D = ("modular_nod", "baseline_mono", "ab_only")
ALL_VARIANTS = VARIANTS_NO_D + VARIANTS_WITH_D


def _make_router() -> Router:
    return Router(RouterConfig(
        d_every_k_steps=0,
        stuck_window=4,
        enable_stuck_trigger=True,
        enable_uncertainty_trigger=True,
        uncertainty_threshold=0.25,
        d_cooldown_steps=8,
    ))


def _random_action(zA: ZA, predict_next_fn, rng: random.Random) -> str:
    """Choose a random non-wall action."""
    valid = []
    for a in ACTIONS:
        zA_next = predict_next_fn(zA, a)
        if zA_next.agent_pos != zA.agent_pos:
            valid.append(a)
    if valid:
        return rng.choice(valid)
    return rng.choice(ACTIONS)


def _check_narrative_hint_mention(narrative: str, coded_hint: str) -> bool:
    """Check if the LLM narrative mentions the coded hint content."""
    if not narrative or not coded_hint:
        return False
    # Check if any part of the coded hint appears in the narrative
    parts = coded_hint.replace("_", " ").split()
    for part in parts:
        if len(part) > 3 and part.lower() in narrative.lower():
            return True
    return False


def _check_narrative_contradiction(
    narrative: str,
    coded_hint: str,
    true_goal_id: str,
    goal_map: Dict,
) -> bool:
    """
    Best-effort detection of whether narrative contradicts the hint.
    E.g. hint says "bottom_right" but narrative says "top left".
    """
    if not narrative:
        return False
    nar_lower = narrative.lower()

    # Check for directional contradictions
    direction_pairs = [
        ("bottom", "top"), ("top", "bottom"),
        ("right", "left"), ("left", "right"),
        ("far", "near"), ("near", "far"),
    ]

    hint_lower = coded_hint.lower()
    for hint_dir, contra_dir in direction_pairs:
        if hint_dir in hint_lower and contra_dir in nar_lower:
            return True

    return False


def run_episode(
    level: AmbiguityLevel,
    variant: str,
    goal_mode: str,
    model: str,
    d_factory=None,
    seed: Optional[int] = None,
) -> LLMAmbiguityResult:
    """
    Run a single episode with coded hints.

    Args:
        level: Test configuration
        variant: Which agent variant to use
        goal_mode: "seek" or "avoid"
        model: "deterministic" or Ollama model name
        d_factory: Callable that creates the D agent (None for no-D variants)
        seed: Random seed
    """
    rng = random.Random(seed)

    # Build base environment
    base_env = GridWorld(
        width=level.width,
        height=level.height,
        seed=seed,
        goals=level.goals,
        hint_cells=level.hint_cells,
        obstacles=[(2, 2)] if level.width >= 5 else [],
        n_random_obstacles=level.n_random_obstacles,
    )

    # Wrap with coded hints
    env = CodedGridWorld(base_env, difficulty=level.difficulty)
    obs = env.reset()

    A = AgentA()
    B = AgentB()

    goal_map = base_env.goal_positions
    true_goal_id = base_env.goal_id  # which goal is the true target

    hint_cell_queue = [h.pos for h in level.hint_cells]
    default_target = (level.width - 1, level.height - 1)

    stay_count = 0
    total_reward = 0.0
    done = False
    d_triggers = 0
    hints_collected = 0
    hints_interpreted = 0
    target_learned = False
    d_latencies: List[float] = []
    format_fallbacks = 0
    total_d_calls = 0
    narrative_mentions_hint = False
    narrative_contradicts_hint = False

    zC = None
    C = None
    D = None
    router = None
    use_d = False
    use_tie_break = False

    # ── Variant setup ──
    if variant in ("modular_nod", "modular_ond_interp", "modular_ond_tb_interp"):
        initial_target = hint_cell_queue[0] if hint_cell_queue else default_target
        zC = ZC(goal_mode=goal_mode, memory={})
        C = AgentC(goal=GoalSpec(mode=goal_mode, target=initial_target), anti_stay_penalty=1.1)
        use_tie_break = (variant == "modular_ond_tb_interp")

    if variant in VARIANTS_WITH_D and d_factory is not None:
        D = d_factory()
        router = _make_router()
        use_d = True

    hint_idx = 0

    for t in range(level.max_steps):
        zA = A.infer_zA(obs)

        # ── Update C's target based on current knowledge ──
        if C is not None and zC is not None:
            if "target" in zC.memory and zC.memory["target"] is not None:
                C.goal.target = tuple(zC.memory["target"])
                target_learned = True
            elif hint_idx < len(hint_cell_queue):
                C.goal.target = hint_cell_queue[hint_idx]
                if zA.agent_pos == hint_cell_queue[hint_idx]:
                    hint_idx += 1
                    if hint_idx < len(hint_cell_queue):
                        C.goal.target = hint_cell_queue[hint_idx]
                    else:
                        C.goal.target = default_target
            else:
                C.goal.target = default_target

        # ── Action Selection ──
        decision_delta = None

        if variant == "baseline_mono":
            target = default_target
            zA_with_goal = ZA(
                width=zA.width, height=zA.height,
                agent_pos=zA.agent_pos, goal_pos=target,
                obstacles=zA.obstacles, hint=zA.hint,
            )
            action = baseline_monolithic_policy(zA_with_goal, mode=goal_mode)

        elif variant == "ab_only":
            action = _random_action(zA, B.predict_next, rng)

        else:
            if use_tie_break:
                action, scored = C.choose_action(zA, B.predict_next, memory=zC.memory, tie_break_delta=0.25)
            else:
                action, scored = C.choose_action(zA, B.predict_next, memory=None, tie_break_delta=0.25)
            decision_delta = scored[0][1] - scored[1][1]

        # ── Environment Step ──
        obs_next, reward, done = env.step(action)
        zA_next = A.infer_zA(obs_next)

        if zA_next.agent_pos == zA.agent_pos:
            stay_count += 1

        # ── Hint Processing ──
        # Uses interpret_hint_only() — deterministic, NO LLM call.
        # The LLM is only invoked via router-gated build_micro() below.
        if zA_next.hint is not None:
            hints_collected += 1

            if use_d and D is not None:
                # Fast path: deterministic hint decoding (no LLM call)
                zD_hint = D.interpret_hint_only(zA_next)

                if zD_hint is not None:
                    # Check if D successfully interpreted the hint
                    for tag in zD_hint.meaning_tags:
                        tag_lower = tag.lower()
                        if (tag_lower.startswith("hint:") and
                                tag_lower[5:] in [g.lower() for g in goal_map]):
                            hints_interpreted += 1
                            break
                        if tag_lower.startswith("not_"):
                            hints_interpreted += 1
                            break

                    if zC is not None:
                        zC = deconstruct_d_to_c(zC, zD_hint, goal_map=goal_map)
                    d_triggers += 1

            elif zC is not None:
                # No-D variant: pass coded hint directly (won't match)
                fake_tags = [f"hint:{zA_next.hint}"]
                zD_hint = ZD(narrative="hint", meaning_tags=fake_tags,
                             length_chars=4, grounding_violations=0)
                zC = deconstruct_d_to_c(zC, zD_hint, goal_map=goal_map)

        total_reward += reward
        obs = obs_next

        # ── D Logic (router-gated) ──
        if use_d and D is not None:
            D.observe_step(t=t, zA=zA_next, action=action, reward=reward, done=done)

            activate_d = False
            if router and decision_delta is not None:
                activate_d, reason = router.should_activate_d(
                    t=t, last_positions=(zA_next.agent_pos,),
                    decision_delta=decision_delta,
                )

            if activate_d:
                d_triggers += 1
                start_t = time.perf_counter()
                zD = D.build_micro(goal_mode=goal_mode, goal_pos=(-1, -1), last_n=5)
                elapsed_ms = (time.perf_counter() - start_t) * 1000.0
                d_latencies.append(elapsed_ms)
                total_d_calls += 1

                if "llm_format_fallback" in zD.meaning_tags:
                    format_fallbacks += 1

                # Narrative quality analysis (LLM-specific)
                if model != "deterministic" and zA_next.hint is not None:
                    coded_hint_str = zA_next.hint or ""
                    if _check_narrative_hint_mention(zD.narrative, coded_hint_str):
                        narrative_mentions_hint = True
                    if _check_narrative_contradiction(zD.narrative, coded_hint_str, true_goal_id, goal_map):
                        narrative_contradicts_hint = True

                if zC is not None:
                    zC = deconstruct_d_to_c(zC, zD, goal_map=goal_map)

        if done:
            break

    steps = (t + 1) if done else level.max_steps
    stay_rate = (stay_count / steps) if steps > 0 else 0.0
    mean_d_latency = mean(d_latencies) if d_latencies else 0.0
    fallback_rate = format_fallbacks / total_d_calls if total_d_calls > 0 else 0.0

    return LLMAmbiguityResult(
        model=model,
        level=level.name,
        difficulty=level.difficulty,
        variant=variant,
        goal_mode=goal_mode,
        success=bool(done),
        steps=steps,
        total_reward=total_reward,
        stay_rate=stay_rate,
        d_triggers=d_triggers,
        hints_collected=hints_collected,
        hints_interpreted=hints_interpreted,
        target_learned=target_learned,
        mean_d_latency_ms=mean_d_latency,
        format_fallback_rate=fallback_rate,
        narrative_mentions_hint=narrative_mentions_hint,
        narrative_contradicts_hint=narrative_contradicts_hint,
    )


# ── Batch Runner ─────────────────────────────────────────────────────

def run_batch(
    n: int = 30,
    goal_mode: str = "seek",
    models: Optional[List[str]] = None,
    include_deterministic: bool = True,
    max_steps_override: Optional[int] = None,
):
    """Run Stufe 6-LLM: architecture robustness study."""
    Path("runs").mkdir(exist_ok=True)
    run_id = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    csv_path = f"runs/llm_semantic_ambiguity_{run_id}.csv"

    levels = _make_levels()
    if max_steps_override is not None:
        for level in levels:
            level.max_steps = min(level.max_steps, max_steps_override)

    # Build model list
    model_list: List[str] = []
    if include_deterministic:
        model_list.append("deterministic")
    if models:
        model_list.extend(models)

    if not model_list:
        print("No models to test. Aborting.")
        return []

    results: List[LLMAmbiguityResult] = []

    try:
        from tqdm import tqdm
        use_tqdm = True
    except ImportError:
        use_tqdm = False

    # Count: no-D variants run once (model-independent), D variants run per model
    n_nod_episodes = len(levels) * len(VARIANTS_NO_D) * n
    n_d_episodes = len(levels) * len(VARIANTS_WITH_D) * n * len(model_list)
    total = n_nod_episodes + n_d_episodes

    if use_tqdm:
        pbar = tqdm(total=total, desc="llm_semantic_ambiguity")

    # ── Run no-D variants once (model-independent) ──
    print(f"\n--- Running no-D variants (model-independent, {len(VARIANTS_NO_D)} variants x {len(levels)} levels x {n} episodes) ---")
    for level in levels:
        _check_encoding(level)
        for variant in VARIANTS_NO_D:
            for i in range(n):
                r = run_episode(
                    level, variant, goal_mode,
                    model="none", d_factory=None, seed=i,
                )
                results.append(r)
                if use_tqdm:
                    pbar.update(1)

    # ── Run D variants per model ──
    for model_name in model_list:
        print(f"\n--- Model: {model_name} ({len(VARIANTS_WITH_D)} variants x {len(levels)} levels x {n} episodes) ---")

        for level in levels:
            goal_map = {g.goal_id: g.pos for g in level.goals}

            for variant in VARIANTS_WITH_D:
                for i in range(n):
                    if model_name == "deterministic":
                        d_factory = lambda gm=goal_map, lv=level: AgentDInterpreter(
                            goal_map=gm,
                            grid_width=lv.width,
                            grid_height=lv.height,
                            difficulty=lv.difficulty,
                        )
                    else:
                        d_factory = lambda gm=goal_map, lv=level, mn=model_name: AgentDLLMInterpreter(
                            llm=OllamaProvider(model=mn),
                            goal_map=gm,
                            grid_width=lv.width,
                            grid_height=lv.height,
                            difficulty=lv.difficulty,
                        )

                    r = run_episode(
                        level, variant, goal_mode,
                        model=model_name, d_factory=d_factory, seed=i,
                    )
                    results.append(r)
                    if use_tqdm:
                        pbar.update(1)

    if use_tqdm:
        pbar.close()

    # Write CSV
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            "model", "level", "difficulty", "variant", "goal_mode",
            "success", "steps", "total_reward", "stay_rate", "d_triggers",
            "hints_collected", "hints_interpreted", "target_learned",
            "mean_d_latency_ms", "format_fallback_rate",
            "narrative_mentions_hint", "narrative_contradicts_hint",
        ])
        for r in results:
            w.writerow([
                r.model, r.level, r.difficulty, r.variant, r.goal_mode,
                r.success, r.steps, f"{r.total_reward:.4f}",
                f"{r.stay_rate:.4f}", r.d_triggers, r.hints_collected,
                r.hints_interpreted, r.target_learned,
                f"{r.mean_d_latency_ms:.1f}", f"{r.format_fallback_rate:.4f}",
                r.narrative_mentions_hint, r.narrative_contradicts_hint,
            ])

    print(f"\nWrote {len(results)} episodes to: {csv_path}")

    _print_results_table(results, levels, model_list)
    _print_robustness_comparison(results, levels, model_list)
    _print_interpretation_stats(results, levels, model_list)
    _print_narrative_analysis(results, levels, model_list)
    _print_latency_analysis(results, model_list)
    _print_def_predictions(results, levels, model_list)

    return results


def _check_encoding(level: AmbiguityLevel):
    """Verify coded hints are unique for this level."""
    goal_map = {g.goal_id: g.pos for g in level.goals}
    encoder = HintEncoder(goal_map, level.width, level.height)
    if not encoder.can_uniquely_identify(level.difficulty):
        print(f"  WARNING: Encoding not unique for {level.name} at {level.difficulty}")


# ── Analysis Functions ───────────────────────────────────────────────

def _print_results_table(
    results: List[LLMAmbiguityResult],
    levels: List[AmbiguityLevel],
    model_list: List[str],
):
    """Print success rates per model x level x variant."""
    print(f"\n{'='*120}")
    print(f"  STUFE 6-LLM: ARCHITECTURE ROBUSTNESS — Success Rate by Model x Level x Variant")
    print(f"{'='*120}")

    d_variants = list(VARIANTS_WITH_D)

    for model_name in model_list:
        print(f"\n  === Model: {model_name} ===")
        for difficulty in ("easy", "medium", "hard"):
            diff_levels = [l for l in levels if l.difficulty == difficulty]
            print(f"\n  --- Difficulty: {difficulty.upper()} ---")

            # Header: no-D variants (from "none" model) + D variants (from this model)
            all_cols = list(VARIANTS_NO_D) + d_variants
            print(f"  {'level':<20s}", end="")
            for v in all_cols:
                label = v[:16]
                print(f" {label:>16s}", end="")
            print()
            print(f"  {'-'*20}", end="")
            for _ in all_cols:
                print(f" {'-'*16}", end="")
            print()

            for level in diff_levels:
                display_name = level.name.rsplit("_", 1)[0]
                print(f"  {display_name:<20s}", end="")

                for v in all_cols:
                    if v in VARIANTS_NO_D:
                        subset = [r for r in results if r.level == level.name
                                  and r.variant == v and r.model == "none"]
                    else:
                        subset = [r for r in results if r.level == level.name
                                  and r.variant == v and r.model == model_name]

                    if subset:
                        sr = sum(1 for r in subset if r.success) / len(subset)
                        _, lo, hi = confidence_interval_proportion(
                            sum(1 for r in subset if r.success), len(subset)
                        )
                        print(f" {sr:>5.3f}[{lo:.2f},{hi:.2f}]", end="")
                    else:
                        print(f" {'N/A':>16s}", end="")
                print()


def _print_robustness_comparison(
    results: List[LLMAmbiguityResult],
    levels: List[AmbiguityLevel],
    model_list: List[str],
):
    """Compare LLM-D vs deterministic-D: the core robustness test."""
    if "deterministic" not in model_list:
        return

    llm_models = [m for m in model_list if m != "deterministic"]
    if not llm_models:
        return

    print(f"\n{'='*120}")
    print(f"  ROBUSTNESS COMPARISON: LLM-D vs Deterministic-D")
    print(f"  (Core claim: stochastic narrative does not disrupt deterministic pipeline)")
    print(f"{'='*120}")

    for variant in VARIANTS_WITH_D:
        print(f"\n  --- Variant: {variant} ---")
        print(f"  {'level':<25s} {'determ':>8s}", end="")
        for m in llm_models:
            print(f" {m[:12]:>12s}", end="")
        print(f"  {'delta':>8s} {'p-val':>8s} {'sig':>5s}")
        print(f"  {'-'*25} {'-'*8}", end="")
        for _ in llm_models:
            print(f" {'-'*12}", end="")
        print(f"  {'-'*8} {'-'*8} {'-'*5}")

        for level in levels:
            det_sub = [r for r in results if r.level == level.name
                       and r.variant == variant and r.model == "deterministic"]
            det_sr = sum(1 for r in det_sub if r.success) / len(det_sub) if det_sub else 0.0

            print(f"  {level.name:<25s} {det_sr:>8.3f}", end="")

            for m in llm_models:
                llm_sub = [r for r in results if r.level == level.name
                           and r.variant == variant and r.model == m]
                if llm_sub:
                    llm_sr = sum(1 for r in llm_sub if r.success) / len(llm_sub)
                    print(f" {llm_sr:>12.3f}", end="")
                else:
                    print(f" {'N/A':>12s}", end="")

            # Statistical comparison: deterministic vs first LLM model
            first_llm = llm_models[0]
            llm_sub = [r for r in results if r.level == level.name
                       and r.variant == variant and r.model == first_llm]
            if det_sub and llm_sub:
                det_vals = [1.0 if r.success else 0.0 for r in det_sub]
                llm_vals = [1.0 if r.success else 0.0 for r in llm_sub]
                report = compare_variants(
                    "deterministic", det_vals, first_llm, llm_vals,
                    "sr", is_proportion=True,
                )
                delta = mean(llm_vals) - mean(det_vals)
                sig = "***" if report["p_value"] < 0.001 else "**" if report["p_value"] < 0.01 else "*" if report["p_value"] < 0.05 else "ns"
                print(f"  {delta:>+8.3f} {report['p_value']:>8.4f} {sig:>5s}")
            else:
                print(f"  {'N/A':>8s} {'N/A':>8s} {'N/A':>5s}")


def _print_interpretation_stats(
    results: List[LLMAmbiguityResult],
    levels: List[AmbiguityLevel],
    model_list: List[str],
):
    """Print hint interpretation statistics per model."""
    print(f"\n{'='*120}")
    print(f"  HINT INTERPRETATION STATISTICS")
    print(f"  (Key: interpretation is deterministic — rate should be 100% for all models)")
    print(f"{'='*120}")
    print(f"  {'model':<18s} {'variant':<22s} {'hints':>6s} {'interp':>7s} {'rate':>6s} {'target':>7s} {'fallback':>9s}")
    print(f"  {'-'*18} {'-'*22} {'-'*6} {'-'*7} {'-'*6} {'-'*7} {'-'*9}")

    for model_name in model_list:
        for variant in VARIANTS_WITH_D:
            subset = [r for r in results if r.model == model_name and r.variant == variant]
            if subset:
                hints = sum(r.hints_collected for r in subset)
                interp = sum(r.hints_interpreted for r in subset)
                rate = interp / hints if hints > 0 else 0.0
                target = sum(1 for r in subset if r.target_learned) / len(subset)
                fb_rate = mean([r.format_fallback_rate for r in subset])
                print(f"  {model_name:<18s} {variant:<22s} {hints:>6d} {interp:>7d} {rate:>6.1%} {target:>7.1%} {fb_rate:>9.1%}")

    # Also show no-D for comparison
    for variant in VARIANTS_NO_D:
        subset = [r for r in results if r.variant == variant and r.model == "none"]
        if subset:
            hints = sum(r.hints_collected for r in subset)
            interp = sum(r.hints_interpreted for r in subset)
            rate = interp / hints if hints > 0 else 0.0
            target = sum(1 for r in subset if r.target_learned) / len(subset)
            print(f"  {'none':<18s} {variant:<22s} {hints:>6d} {interp:>7d} {rate:>6.1%} {target:>7.1%} {'N/A':>9s}")


def _print_narrative_analysis(
    results: List[LLMAmbiguityResult],
    levels: List[AmbiguityLevel],
    model_list: List[str],
):
    """Analyze LLM narrative quality signals."""
    llm_models = [m for m in model_list if m != "deterministic"]
    if not llm_models:
        return

    print(f"\n{'='*120}")
    print(f"  NARRATIVE QUALITY ANALYSIS (LLM models only)")
    print(f"  (Secondary: does the LLM narrative mention or contradict the coded hint?)")
    print(f"{'='*120}")
    print(f"  {'model':<18s} {'mentions':>10s} {'contradicts':>12s} {'episodes':>10s}")
    print(f"  {'-'*18} {'-'*10} {'-'*12} {'-'*10}")

    for model_name in llm_models:
        d_subset = [r for r in results if r.model == model_name
                    and r.variant in VARIANTS_WITH_D]
        if d_subset:
            mentions = sum(1 for r in d_subset if r.narrative_mentions_hint)
            contradicts = sum(1 for r in d_subset if r.narrative_contradicts_hint)
            total_ep = len(d_subset)
            print(f"  {model_name:<18s} {mentions:>7d}/{total_ep:<3d} {contradicts:>9d}/{total_ep:<3d} {total_ep:>10d}")


def _print_latency_analysis(
    results: List[LLMAmbiguityResult],
    model_list: List[str],
):
    """Print D-call latency per model."""
    print(f"\n{'='*120}")
    print(f"  D-CALL LATENCY ANALYSIS")
    print(f"{'='*120}")
    print(f"  {'model':<18s} {'mean_ms':>10s} {'episodes':>10s}")
    print(f"  {'-'*18} {'-'*10} {'-'*10}")

    for model_name in model_list:
        d_subset = [r for r in results if r.model == model_name
                    and r.variant in VARIANTS_WITH_D
                    and r.mean_d_latency_ms > 0]
        if d_subset:
            avg_lat = mean([r.mean_d_latency_ms for r in d_subset])
            print(f"  {model_name:<18s} {avg_lat:>10.1f} {len(d_subset):>10d}")


def _print_def_predictions(
    results: List[LLMAmbiguityResult],
    levels: List[AmbiguityLevel],
    model_list: List[str],
):
    """Check DEF predictions for architecture robustness."""
    print(f"\n{'='*120}")
    print(f"  DEF PREDICTIONS CHECK — Architecture Robustness")
    print(f"{'='*120}")

    all_pass = True

    # ── Prediction 1: Pipeline robustness (LLM SR ≈ deterministic SR) ──
    print(f"\n  Prediction 1: Pipeline robustness — LLM-D SR identical to deterministic-D SR")
    llm_models = [m for m in model_list if m != "deterministic"]

    if "deterministic" in model_list and llm_models:
        for variant in VARIANTS_WITH_D:
            det_sub = [r for r in results if r.model == "deterministic" and r.variant == variant]
            det_sr = sum(1 for r in det_sub if r.success) / len(det_sub) if det_sub else 0.0

            for m in llm_models:
                llm_sub = [r for r in results if r.model == m and r.variant == variant]
                llm_sr = sum(1 for r in llm_sub if r.success) / len(llm_sub) if llm_sub else 0.0

                delta = abs(llm_sr - det_sr)
                passed = delta < 0.10  # within 10%
                status = "PASS" if passed else "FAIL"
                if not passed:
                    all_pass = False
                print(f"    {variant} — det={det_sr:.3f} vs {m}={llm_sr:.3f} delta={delta:.3f} [{status}]")
    else:
        print(f"    (skipped — need both deterministic and LLM models)")

    # ── Prediction 2: Interpretation isolation (100% for all models) ──
    print(f"\n  Prediction 2: Interpretation isolation — hints_interpreted = 100% for all models")
    for model_name in model_list:
        d_sub = [r for r in results if r.model == model_name and r.variant in VARIANTS_WITH_D]
        hints = sum(r.hints_collected for r in d_sub)
        interp = sum(r.hints_interpreted for r in d_sub)
        rate = interp / hints if hints > 0 else 0.0
        passed = rate > 0.95
        status = "PASS" if passed else "FAIL"
        if not passed:
            all_pass = False
        print(f"    {model_name}: {rate:.1%} ({interp}/{hints}) [{status}]")

    # ── Prediction 3: Format-fallback isolation ──
    print(f"\n  Prediction 3: Format-fallback isolation — hint interpretation works despite LLM format errors")
    for m in llm_models if llm_models else []:
        d_sub = [r for r in results if r.model == m and r.variant in VARIANTS_WITH_D]
        fb_episodes = [r for r in d_sub if r.format_fallback_rate > 0]
        if fb_episodes:
            fb_hints = sum(r.hints_collected for r in fb_episodes)
            fb_interp = sum(r.hints_interpreted for r in fb_episodes)
            fb_rate = fb_interp / fb_hints if fb_hints > 0 else 0.0
            passed = fb_rate > 0.95
            status = "PASS" if passed else "FAIL"
            if not passed:
                all_pass = False
            print(f"    {m}: {len(fb_episodes)} episodes with fallback, interp rate={fb_rate:.1%} [{status}]")
        else:
            print(f"    {m}: 0 episodes with fallback (no format errors) [PASS by default]")

    # ── Prediction 4: D > no-D holds model-independently ──
    print(f"\n  Prediction 4: D > no-D holds model-independently")
    nod_sub = [r for r in results if r.variant == "modular_nod" and r.model == "none"]
    nod_sr = sum(1 for r in nod_sub if r.success) / len(nod_sub) if nod_sub else 0.0

    for model_name in model_list:
        d_sub = [r for r in results if r.model == model_name and r.variant == "modular_ond_interp"]
        d_sr = sum(1 for r in d_sub if r.success) / len(d_sub) if d_sub else 0.0
        passed = d_sr > nod_sr
        status = "PASS" if passed else "FAIL"
        if not passed:
            all_pass = False
        print(f"    {model_name}: D={d_sr:.3f} vs noD={nod_sr:.3f} gap={d_sr - nod_sr:+.3f} [{status}]")

    # ── Prediction 5: Narrative quality signal (informational) ──
    print(f"\n  Prediction 5: Narrative quality signal (informational, not pass/fail)")
    if "deterministic" in model_list and llm_models:
        for variant in VARIANTS_WITH_D:
            det_sub = [r for r in results if r.model == "deterministic" and r.variant == variant]
            det_sr = sum(1 for r in det_sub if r.success) / len(det_sub) if det_sub else 0.0

            for m in llm_models:
                llm_sub = [r for r in results if r.model == m and r.variant == variant]
                llm_sr = sum(1 for r in llm_sub if r.success) / len(llm_sub) if llm_sub else 0.0

                delta = llm_sr - det_sr
                direction = "LLM better" if delta > 0.01 else ("LLM worse" if delta < -0.01 else "equivalent")
                print(f"    {variant} — {m}: delta={delta:+.3f} ({direction})")
    else:
        print(f"    (skipped — need both deterministic and LLM models)")

    print(f"\n  Overall: {'ALL PASS' if all_pass else 'SOME PREDICTIONS NOT MET'}")


# ── CLI ──────────────────────────────────────────────────────────────

def main():
    parser = argparse.ArgumentParser(
        description="Stufe 6-LLM: Architecture Robustness with Stochastic Narrative"
    )
    parser.add_argument("--n", type=int, default=30,
                        help="Episodes per level x variant (default: 30)")
    parser.add_argument("--model", type=str, default=None,
                        help="Single Ollama model to test")
    parser.add_argument("--models", nargs="+", default=None,
                        help="Specific Ollama models to test")
    parser.add_argument("--goal-mode", type=str, default="seek",
                        choices=["seek", "avoid"])
    parser.add_argument("--no-deterministic", action="store_true",
                        help="Skip deterministic control group")
    parser.add_argument("--max-steps", type=int, default=None,
                        help="Override max_steps per level (useful to cap LLM latency)")
    args = parser.parse_args()

    # Check Ollama
    ollama_ok = check_ollama_available()

    # Resolve models
    if args.model:
        models = [args.model]
    elif args.models:
        models = args.models
    elif ollama_ok:
        models = get_available_test_models()
        if not models:
            print("Ollama running but no models available. Using deterministic only.")
            models = []
    else:
        print("Ollama not available. Using deterministic only.")
        models = []

    if not models and args.no_deterministic:
        print("ERROR: No LLM models available and --no-deterministic set. Nothing to run.")
        sys.exit(1)

    print(f"Stufe 6-LLM: Architecture Robustness with Stochastic Narrative")
    print(f"  Models: {['deterministic'] + models if not args.no_deterministic else models}")
    print(f"  Episodes per combination: {args.n}")
    print(f"  Levels: {len(_make_levels())}")
    print(f"  Variants: {list(ALL_VARIANTS)}")

    run_batch(
        n=args.n,
        goal_mode=args.goal_mode,
        models=models,
        include_deterministic=not args.no_deterministic,
        max_steps_override=args.max_steps,
    )


if __name__ == "__main__":
    main()
